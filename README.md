# Data Lake and Datalakehouse Implementation Projects

This repository contains a series of Projects that implement Data Lake and Datalakehouse.

## Overview

in creation

<br/><br/>
Project1 - In this project we built an AWS Data Lake environment with all data implementation steps. (extraction, upload - AWS, lineage, observability, quality, enrichment and governance). In the end, everything will be automated to run daily with incremental loads.

<br/><br/>
Project2 - 

<br/><br/>
Project3 - Project2 - This project used the features of Delta Lake, a powerful storage layer for Data Lakes, focusing on CRUD (Create, Read, Update, Delete) operations and the Time Travel feature. Thus, we were able to manipulate data efficiently, make incremental changes to Delta tables, and consult historical versions of the data for auditing and recovery. An initial data mass was created, update and delete operations were performed, new records were inserted, and analyses were performed with filters and groupings, all while ensuring the reliability and intrinsic versioning of Delta Lake.

<br/><br/>
project4 - 

<br/><br/>
Project5 - 


<br/><br/>

## Data Lake and Datalakehouse

 <img width="2500px" align="right"  src="https://github.com/julianasantimaria/Projects_DataLakeAndDatalakehouseImplementation/blob/HTML/image.jpg">

 <br/>
 <br/>
 <br/><br/><br/><br/><br/><br/>


## Project Structure

in creation


## Requisites

Make sure you have Docker and accounts in the Azure, AWS and GCP clouds.

```bash
- Docker
- AWS (CloudFormation and S3)
- Databricks
- SQL
- PySpark